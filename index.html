<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <title>LiveAvatar Minimal Client</title>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <style>
    body { font-family: system-ui, sans-serif; padding: 1rem; }
    video { width: 100%; max-width: 720px; background: #000; border-radius: 8px; }
    .controls { display: flex; gap: 8px; margin-top: 12px; }
    button { padding: 8px 12px; }
    .log { font-family: ui-monospace, SFMono-Regular, Menlo, monospace; white-space: pre-wrap; margin-top: 12px; }
  </style>
</head>
<body>
  <h1>LiveAvatar Stream</h1>
  <video id="remoteVideo" playsinline autoplay muted></video>
  <div class="controls">
    <button id="connectBtn">Connect</button>
    <button id="disconnectBtn" disabled>Disconnect</button>
  </div>
  <div class="log" id="log"></div>

  <script>
    const SIGNALING_URL = "wss://YOUR_BACKEND/signaling"; // <-- change this
    const ICE_SERVERS = [
      { urls: "stun:stun.l.google.com:19302" },
      // If your backend provides TURN:
      // { urls: "turn:turn.example.com:3478", username: "user", credential: "pass" },
    ];

    const logEl = document.getElementById("log");
    const v = document.getElementById("remoteVideo");
    const connectBtn = document.getElementById("connectBtn");
    const disconnectBtn = document.getElementById("disconnectBtn");

    let pc = null;
    let ws = null;

    function log(msg) {
      console.log(msg);
      logEl.textContent += `[${new Date().toLocaleTimeString()}] ${msg}\n`;
    }

    async function start() {
      connectBtn.disabled = true;
      disconnectBtn.disabled = false;

      // 1) Create RTCPeerConnection
      pc = new RTCPeerConnection({ iceServers: ICE_SERVERS });

      // 2) Handle remote tracks (LiveAvatar video/audio)
      const remoteStream = new MediaStream();
      v.srcObject = remoteStream;
      pc.addEventListener("track", (ev) => {
        log(`Remote track: kind=${ev.track.kind}`);
        remoteStream.addTrack(ev.track);
      });

      // 3) Optional: upstream mic if your LiveAvatar needs user audio
      // try {
      //   const mic = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
      //   mic.getTracks().forEach(t => pc.addTrack(t, mic));
      // } catch (e) {
      //   log("Mic disabled or unavailable, continuing without upstream audio.");
      // }

      // 4) Connect WebSocket for signaling
      ws = new WebSocket(SIGNALING_URL);
      ws.onopen = async () => {
        log("Signaling connected.");
        // 5) Create data channel (optional, for text commands to avatar)
        const dc = pc.createDataChannel("commands");
        dc.onopen = () => log("Data channel open.");
        dc.onmessage = (e) => log("DC message: " + e.data);

        // 6) Exchange ICE candidates through WS
        pc.onicecandidate = (ev) => {
          if (ev.candidate && ws.readyState === WebSocket.OPEN) {
            ws.send(JSON.stringify({ type: "candidate", candidate: ev.candidate }));
          }
        };

        // 7) Create and send Offer
        const offer = await pc.createOffer({
          offerToReceiveAudio: true,
          offerToReceiveVideo: true,
        });
        await pc.setLocalDescription(offer);
        ws.send(JSON.stringify({ type: "offer", sdp: offer.sdp }));
        log("Offer sent.");
      };

      ws.onmessage = async (message) => {
        const data = JSON.parse(message.data);
        if (data.type === "answer") {
          log("Answer received.");
          await pc.setRemoteDescription({ type: "answer", sdp: data.sdp });
        } else if (data.type === "candidate" && data.candidate) {
          log("Remote ICE candidate received.");
          try {
            await pc.addIceCandidate(data.candidate);
          } catch (err) {
            log("Error adding ICE candidate: " + err);
          }
        } else if (data.type === "error") {
          log("Server error: " + data.message);
        }
      };

      ws.onerror = (e) => log("WS error: " + JSON.stringify(e));
      ws.onclose = () => log("Signaling closed.");
    }

    async function stop() {
      disconnectBtn.disabled = true;
      connectBtn.disabled = false;

      if (ws && ws.readyState === WebSocket.OPEN) ws.close();
      ws = null;

      if (pc) {
        pc.getSenders().forEach(s => { try { s.track && s.track.stop(); } catch {} });
        pc.getReceivers().forEach(r => { try { r.track && r.track.stop(); } catch {} });
        pc.onicecandidate = null;
        pc.ontrack = null;
        pc.close();
      }
      pc = null;

      // Clear video
      v.pause();
      v.srcObject = null;
      log("Disconnected.");
    }

    connectBtn.addEventListener("click", start);
    disconnectBtn.addEventListener("click", stop);

    // Autoplay policies: muted attribute is set. User interaction (click) also helps.
    // If your browser blocks autoplay with sound, keep `muted` on the <video>.
  </script>
</body>
</html>
