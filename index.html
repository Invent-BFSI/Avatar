<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Azure Real-time Avatar (WebRTC)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body { font-family: system-ui, sans-serif; margin: 0; padding: 16px; }
    #video { width: 100%; max-width: 960px; aspect-ratio: 16/9; background: #000; }
    #controls { margin-top: 12px; display: flex; gap: 8px; }
    input[type="text"] { flex: 1; padding: 8px; }
    button { padding: 8px 12px; }
    #log { font-size: 12px; white-space: pre-wrap; background: #f6f8fa; margin-top: 12px; padding: 8px; }
  </style>
</head>
<body>
  <h1>Azure Real-time Avatar</h1>

  <video id="video" autoplay playsinline muted></video>
  <audio id="audio" autoplay></audio>

  <div id="controls">
    <input id="region" placeholder="Speech region e.g. westus2" />
    <input id="key" placeholder="(Dev only) Speech key" />
    <button id="start">Start Session</button>
  </div>

  <div id="speakRow" style="margin-top:8px; display:none;">
    <input id="text" placeholder="Type something for the avatar to say…" />
    <button id="speak">Speak</button>
    <button id="stop">Stop</button>
    <button id="close">Close Session</button>
  </div>

  <div id="log"></div>

  <!-- Load Speech SDK (official) -->
  <script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>

  <script>
    const $ = (id) => document.getElementById(id);
    const log = (...a) => { console.log(...a); $('log').textContent += a.join(' ') + '\n'; };

    let peer, avatarSynthesizer;

    async function getRelay() {
      const r = await fetch('/api/avatar/relay-token');
      if (!r.ok) throw new Error('Failed to fetch relay token');
      return r.json(); // { urls:[...], username:'...', credential:'...', ttl:'...' }
    }

    async function startSession() {
      const region = $('region').value.trim();
      const key = $('key').value.trim();

      if (!region || !key) {
        return alert('Enter your Speech resource region and key (for local dev).\nIn production, mint a short-lived Speech token server-side.');
      }

      log('Fetching ICE relay details…');
      const relay = await getRelay();
      const iceServers = [{
        // Use only TURN URL per docs; STUN/turn both may be returned, but TURN is recommended for reliable media relay.
        urls: relay.urls.filter(u => u.startsWith('turn:')),
        username: relay.username,
        credential: relay.credential
      }];

      // 1) Create WebRTC PeerConnection with ICE (TURN) servers
      peer = new RTCPeerConnection({ iceServers });
      peer.oniceconnectionstatechange = () => log('ICE state:', peer.iceConnectionState);

      // 2) Wire remote tracks to <video>/<audio>
      peer.ontrack = (event) => {
        if (event.track.kind === 'video') {
          $('video').srcObject = event.streams[0];
        }
        if (event.track.kind === 'audio') {
          $('audio').srcObject = event.streams[0];
        }
      };
      // Offer to receive one video and one audio track (bidirectional by default)
      peer.addTransceiver('video', { direction: 'sendrecv' });
      peer.addTransceiver('audio', { direction: 'sendrecv' });

      // 3) Create Speech + Avatar configs
      const SpeechSDK = window.SpeechSDK;
      const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(key, region);
      // Choose a voice (you can change it later)
      speechConfig.speechSynthesisVoiceName = "en-US-JennyNeural";

      // Video format can be customized (crop/resize), but defaults to 16:9 1080p
      const avatarConfig = new SpeechSDK.AvatarConfig(
        "lisa",          // character
        "casual-sitting" // style
      );
      // Example background color (green for client-side matting if you want)
      avatarConfig.backgroundColor = "#00FF00FF";

      // 4) Start the avatar (this negotiates WebRTC with the service)
      log('Starting avatar…');
      avatarSynthesizer = new SpeechSDK.AvatarSynthesizer(speechConfig, avatarConfig);

      await avatarSynthesizer.startAvatarAsync(peer)
        .then(() => log('Avatar started. You should see idle video.'))
        .catch((err) => { throw new Error('Avatar failed to start: ' + err); });

      // UI
      $('speakRow').style.display = 'flex';
      $('video').muted = false; // unmute after user gesture
    }

    async function speak() {
      const text = $('text').value.trim();
      if (!text) return;
      try {
        const result = await avatarSynthesizer.speakTextAsync(text);
        if (result.reason === SpeechSDK.ResultReason.SynthesizingAudioCompleted) {
          log('Synthesis complete.');
        } else {
          log('Synthesis did not complete. ResultId:', result.resultId);
        }
      } catch (e) {
        log('Speak error:', e.message);
      }
    }

    async function stopSpeaking() {
      try { await avatarSynthesizer.stopSpeakingAsync(); } catch {}
    }

    function closeSession() {
      try { avatarSynthesizer?.close(); } catch {}
      try { peer?.close(); } catch {}
      log('Session closed.');
      $('speakRow').style.display = 'none';
    }

    $('start').addEventListener('click', startSession);
    $('speak').addEventListener('click', speak);
    $('stop').addEventListener('click', stopSpeaking);
    $('close').addEventListener('click', closeSession);
  </script>
</body>
</html>
